{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLzWw_0mPShA"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gtN-XKAOlLR"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "import regex as re\n",
        "import nltk\n",
        "def tokensize(text):\n",
        "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*',text)\n",
        "\n",
        "nltk.download(\"all\")\n",
        "\n",
        "     \n",
        "\n",
        "un = pd.read_csv(\"/content/drive/MyDrive/UN/un-general-debates.csv\")\n",
        "\n",
        "     \n",
        "\n",
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def remove_stop(tokens):\n",
        "    return [t for t in tokens if t.lower() not in stopwords]\n",
        "     \n",
        "\n",
        "pipeline = [str.lower,tokensize,remove_stop]\n",
        "\n",
        "def prepare(text, pipeline):\n",
        "    tokens = text\n",
        "    for transform in pipeline:\n",
        "        tokens = transform(tokens)\n",
        "    return tokens\n",
        "     \n",
        "\n",
        "un['tokens'] = un['text'].apply(prepare, pipeline=pipeline)\n",
        "un['num_tokens'] = un['tokens'].map(len) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v9F2nNGPOph"
      },
      "source": [
        "# N-gram Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrR-_rkLPmjg"
      },
      "source": [
        "동일 단어를 활용해도 단어의 순서 혹은 합성어에 따라서 의미는 달라진다.\n",
        "\n",
        ">해결: 빈도 분석의 대상을 단일 단어에서 두 세 단어의 짧은 시퀀스로 확장하는 것이 도움된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKqYrbsVP4MJ"
      },
      "source": [
        "합성어와 연어라는 두 가지 유형의 단어 시퀀스를 찾는데 합성어는 특정한 의미를 지닌 두 개 이상의 단어가 조합된 말이다.\n",
        "\n",
        "그러므로, 두 개의 토큰을 하나의 의미로 간주해야 하고 이와 대조적으로 연어란 자주 사용되는 단어 조합이다.\n",
        "\n",
        "그러므로, 텍스트 처리에는 주로 두 단어의 조합, 세 단어의 조합으로 작업한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyc8Te4SQWBB"
      },
      "source": [
        "N-gram의 크기가 1이면 단일 단어이며 유니그램으로 부르고, n<=3을 고수하는 이유 n값이 커질수록 서로 다른 N-그램의 수는 기하급수적으로 증가하며 빈도는 기하급수적으로 감소하기 때문이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knriz4RCPRhf",
        "outputId": "9dea28db-3dbc-4358-a92a-4b05149f525b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the visible|visible mainfestation|mainfestation of|of the|the global|global climate|climate change\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "바이어그램에는 대부분의 전치사 및 한정사같은 불용어가 포함되므로 불용어없이 바이어그램을 작성하는 것이 좋다.\n",
        "불용어를 제거한 다음에 바이그램을 빌드하면 원본텍스트에 존재하지 않는 바이그램이 생성되므로 수정된 ngrams함수를 사용해서 \n",
        "모든 토큰에 대한 바이그램을 생성한 뒤 불용어를 포함하지 않는 토큰만 유지한다.\n",
        "\"\"\"\n",
        "def ngrams(tokens, n=2, sep=' '):\n",
        "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])]\n",
        "\n",
        "text = \"the visible mainfestation of the global climate change\"\n",
        "tokens = tokensize(text)\n",
        "print(\"|\".join(ngrams(tokens,2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMDCTY2FQ9vh",
        "outputId": "0ee1c2c7-92c8-45c1-e28b-3ce7dc05e5ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bigrams: visible mainfestation|global climate|climate change\n",
            "Trigrams: global climate change\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "ngrams 함수는 모든 바이그램을 포함한 열을 데이터프레임에 추가\n",
        "앞서 단어 계수에 사용한 count_words를 적용해 상위 5개 바이그램 결정\n",
        "\"\"\"\n",
        "def ngrams(tokens, n=2, sep=' ', stopwords=set()):\n",
        "    return [sep.join(ngram) for ngram in zip(*[tokens[i:] for i in range(n)])\n",
        "            if len([t for t in ngram if t in stopwords])==0]\n",
        "print(\"Bigrams:\", \"|\".join(ngrams(tokens,2,stopwords=stopwords)))\n",
        "print(\"Trigrams:\", \"|\".join(ngrams(tokens,3,stopwords=stopwords)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WMpn08zRR1-o"
      },
      "outputs": [],
      "source": [
        "un['bigrams'] = un['text'].apply(prepare, pipeline=[str.lower, tokensize]) \\\n",
        "                          .apply(ngrams,n=2, stopwords=stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E4Ol39ajTVGL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "자세하게 추가 처리 및 분석을 위해선 Counter 클래스를 팬더스 데이터프레임으로 변환하는 것이 훨씬 더 편하다\n",
        "토큰은 데이터프레임의 인덱스가 되고, 빈도값은 freq라는 열에 저장되고, 가장 자주 사용되는 단어가 행의 첫 부분에 표시되도록 정렬된다.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "판다스의 데이터프레임을 첫 번째 매개변수로 사용하고, 토큰 혹은 텍스트를 포함한 열 이름을 두 번째 매개변수로 사용한다.\n",
        "\n",
        "데이터프레임의 토큰 열에 연설을 토큰화한 토큰이 저장\n",
        "\"\"\"\n",
        "\n",
        "from collections import Counter\n",
        "counter = Counter()\n",
        "def count_words(df, column=\"tokens\", preprocess=None, min_freq=2):\n",
        "\n",
        "    #토큰들을 처리하고 counter를 업데이트한다.\n",
        "    def update(doc):\n",
        "        tokens =doc if preprocess is None else preprocess(doc)\n",
        "        counter.update(tokens)\n",
        "\n",
        "        #counter를 생성하고, 모든 데이터에 대해 update를 실행한다.\n",
        "    counter = Counter()\n",
        "    df[column].map(update)\n",
        "\n",
        "        #counter를 DataFrame화\n",
        "    freq_df = pd.DataFrame.from_dict(counter, orient=\"index\",columns=['freq'])\n",
        "    freq_df = freq_df.query('freq >= @min_freq') #빈도수가 최솟값(0) 이상인 것들을 뽑아라\n",
        "    freq_df.index.name = 'token'\n",
        "    return freq_df.sort_values(\"freq\", ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "K9Qh-tk6Ss3Y",
        "outputId": "29893de2-b23f-4e9e-ac2d-7b8a59dbfad8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                           freq\n",
              "token                          \n",
              "united nations           103236\n",
              "international community   27786\n",
              "general assembly          27096\n",
              "security council          20961\n",
              "human rights              19856"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6bb9b345-caa2-4c0a-813e-ca81ce238b7d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>token</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>united nations</th>\n",
              "      <td>103236</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>international community</th>\n",
              "      <td>27786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>general assembly</th>\n",
              "      <td>27096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>security council</th>\n",
              "      <td>20961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>human rights</th>\n",
              "      <td>19856</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6bb9b345-caa2-4c0a-813e-ca81ce238b7d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-6bb9b345-caa2-4c0a-813e-ca81ce238b7d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-6bb9b345-caa2-4c0a-813e-ca81ce238b7d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "count_words(un, 'bigrams').head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWKPiXh-TiGZ"
      },
      "source": [
        "문장 경계 무시로 인해서 생성되는 이상한 바이그램은 실제론 중요하지 않다.\n",
        "\n",
        "그러나, 이를 방지하려면 문장 경계를 식별해야하는데 토큰화보다 더 복잡하다.\n",
        "\n",
        "TF-IDF 기반의 유니그램 분석을 확장해 바이그램을 포함하고, 바이그램의 idf값을 추가하여 모든 연설에 대해 가중 바이그램 빈도를 계산한 결과 데이터프레임에서 워드 클라우드를 생성한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HX0DaDLXTEaP"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "count_words와 거의 동일하지만, 각 토큰이 문서당 한 번만 계산 (counter.update(set(tokens)))된다.\n",
        "\n",
        "IDF값이 용어의 총 개수를 계산한 후에 계산되므로 매개변수 mid_df는 사용빈도가 낮은 롱테일에 대한 필터 역할을 한다.\n",
        "\n",
        "아래의 함수는 데이터프레임을 반환한다.\n",
        "\"\"\"\n",
        "\n",
        "def compute_idf(df, column='tokens', preprocess=None, min_df=2):\n",
        "\n",
        "    def update(doc):\n",
        "        tokens = doc if preprocess is None else preprocess(doc)\n",
        "        counter.update(set(tokens))\n",
        "\n",
        "    #토큰 개수를 얻는다.\n",
        "    counter = Counter()\n",
        "    df[column].map(update)\n",
        "\n",
        "    #데이터프레임 생성 후 idf를 계산한다.\n",
        "\n",
        "    idf_df = pd.DataFrame.from_dict(counter, orient='index',columns=['df'])\n",
        "    idf_df = idf_df.query('df >= @min_df')\n",
        "    idf_df['idf'] = np.log(len(df)/idf_df['df']) + 0.1\n",
        "    idf_df.index.name = 'token'\n",
        "    return idf_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ7L47cqUOqn",
        "outputId": "be229060-edec-40aa-f4b7-cf6288a3cb24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-6fc370bd1dce>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  idf_df['idf'] = np.log(len(df)/idf_df['df']) + 0.1\n"
          ]
        }
      ],
      "source": [
        "idf_df = compute_idf(un)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "count_words에 의해 생성된 빈도값을 포함하는 판다스의 시리즈도 지원하도록\n",
        "함수를 둘러싼 작은 래퍼를 준비하였고, WordCloud클래스에는 결과를 미세 조정할 수 있는 옵션이 존재하여,\n",
        "이 중 일부를 사용해서 결과를 조정한다.\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "아래 함수는 단어를 필터링하는 편의 매개 변수가 두 개가 있는데 skip_n은 목록의 상위 n개 단어를 건너뛴다.\n",
        "\n",
        "유엔 말뭉치에는 분명 united, Nations, Interational같은 단어가 빈도 목록의 앞에 나와서\n",
        "나열된 단어를 시각화하는 것이 더 흥미롭다.\n",
        "\n",
        "두 번째 필터는 불용어를 추가한 목록으로 자주 사용되지만 시각화를 위해 흥미롭지 않은 특정 단어를 필터링 하는 것이 도움이 된다.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def wordcloud(word_freq, title=None, max_words=200, stopwords=None):\n",
        "\n",
        "    wc = WordCloud(width=800, height=400,\n",
        "                   background_color=\"black\", colormap=\"Paired\",\n",
        "                   max_font_size=150, max_words=max_words)\n",
        "    \n",
        "    #데이터프레임을 사전형으로 변경한다.\n",
        "    if type(word_freq) == pd.Series:\n",
        "        counter = Counter(word_freq.fillna(0).to_dict())\n",
        "    else:\n",
        "        counter = word_freq\n",
        "\n",
        "    #빈도 counter에서 불용어를 필터링한다.\n",
        "    if stopwords is not None:\n",
        "        counter = {token:freq for (token,freq) in counter.items()\n",
        "                              if token not in stopwords}\n",
        "    wc.generate_from_frequencies(counter)\n",
        "\n",
        "    plt.title(title)\n",
        "    \n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "KYDy2MuB03_S"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olciKSnkUSdj"
      },
      "outputs": [],
      "source": [
        "idf_df = pd.concat([idf_df, compute_idf(un,'bigrams',min_df=10)])\n",
        "\n",
        "freq_df = count_words(un[un['year'] ==2015], 'bigrams')\n",
        "freq_df['tfidf'] = freq_df['freq'] + idf_df['idf']\n",
        "wordcloud(freq_df['tfidf'], title='all bigrams', max_words=50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "where = freq_df.index.str.contains(\"climate\")\n",
        "wordcloud(freq_df[where]['freq'], title='\"climate\" bigrams', max_words=50)"
      ],
      "metadata": {
        "id": "E8JGd60J1bUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "불용어를 포함하지 않은 모든 N-그램을 생성하고 가중치를 부여하는데 이 결과는 상당히 좋다.\n",
        "\n",
        "<br>\n",
        "\n",
        "드물게 나타나는 롱테일 바이그램은 신경 쓰지 않는다.\n",
        "\n",
        "NLTK의 언어 감지는 계산 비용이 많이 들지만 더 정교한 알고리즘을 사용해 연어를 식별한다."
      ],
      "metadata": {
        "id": "y4zL_Tpa2JI_"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1bWBOVQ2LiAvQgcM4oznLTXgSkMj4GceB",
      "authorship_tag": "ABX9TyMPP37L6FyytA2dnIaEyz2N"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}