{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1CfQo6zOyYTm5nCr50kauKqykUBUORdof",
      "authorship_tag": "ABX9TyM5KCkwFUlea8ZPIZAWFbzB"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "CiSaThhmL4lt"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import regex as re\n",
        "import nltk\n",
        "def tokensize(text):\n",
        "    return re.findall(r'[\\w-]*\\p{L}[\\w-]*',text)\n",
        "\n",
        "nltk.download(\"all\")\n"
      ],
      "metadata": {
        "id": "hfzUwvacL6lO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "un = pd.read_csv(\"/content/drive/MyDrive/UN/un-general-debates.csv\")\n"
      ],
      "metadata": {
        "id": "tiZHSjQKMCJh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "def remove_stop(tokens):\n",
        "    return [t for t in tokens if t.lower() not in stopwords]\n",
        "     \n",
        "\n",
        "pipeline = [str.lower,tokensize,remove_stop]\n",
        "\n",
        "def prepare(text, pipeline):\n",
        "    tokens = text\n",
        "    for transform in pipeline:\n",
        "        tokens = transform(tokens)\n",
        "    return tokens\n",
        "     \n",
        "\n",
        "un['tokens'] = un['text'].apply(prepare, pipeline=pipeline)\n",
        "un['num_tokens'] = un['tokens'].map(len)        "
      ],
      "metadata": {
        "id": "Fmcq_DhlMIBv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 컨텍스트 내 키워드 탐색\n",
        "\n",
        ">워드 클라우드와 빈도 다이어그램은 텍스트 데이터를 시각적으로 요약하는 훌륭한 도구지만 특정 용어가 왜 그렇게 두드러지게 등장하는지는 의문을 제기하기도 한다.\n",
        "\n",
        "<br>\n",
        "\n",
        "실제로, 내용을 이해하려면 전처리되지 않은 원본 텍스트에서 해당 단어가 사용된 곳을 직접 찾아 확인해야하는데 분량이 많을 것이다.\n",
        "\n",
        "이를 영리하게 처리하는 방법은 컨텍스트 내 키워드(KWIC) 분석법이다.\n",
        "\n",
        "이는, 키워드를 중심으로 왼쪽 및 오른쪽에 있는 일정한 개수의 단어를 목록화한다."
      ],
      "metadata": {
        "id": "DAeGmIlWM5Ky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KWIC 분석\n",
        "\n",
        "- NLTK 및 textacy로 구현된다\n",
        "\n",
        "- textacy의 KWIC함수를 사용하고 이 함수는 토큰화되지 않은 텍스트에서 잘 작동하며 빠르게 실행되고, climate change와 같이 여러 토큰에 걸쳐 있는 문자열을 검색할 수 있다.(NLTK는 검색할 수 없다.) NLTK와 textacy의 KWIC함수는 모두 단일 문서에서만 작동하고, 데이터프레임의 여러 문서에서 해석이 가능하도록 확장하겠다."
      ],
      "metadata": {
        "id": "KCr5YzR4ODTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy\n"
      ],
      "metadata": {
        "id": "FFKAy4nvQFJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy\n",
        "\n",
        "if textacy.__version__ < '0.11': # as in printed book\n",
        "    from textacy.text_utils import KWIC\n",
        "    \n",
        "else: # for textacy 0.11.x\n",
        "    from textacy.extract.kwic import keyword_in_context\n",
        "\n",
        "    def KWIC(*args, **kwargs):\n",
        "        # call keyword_in_context with all params except 'print_only'\n",
        "        return keyword_in_context(*args, \n",
        "                           **{kw: arg for kw, arg in kwargs.items() \n",
        "                            if kw != 'print_only'})"
      ],
      "metadata": {
        "id": "okqgOAeXSG_7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "def kwic(doc_series, keyword, window=35, print_samples=5):\n",
        "\n",
        "    def add_kwic(text):\n",
        "        kwic_list.extend(KWIC(text, keyword, ignore_case=True, \n",
        "                              window_width=window, print_only=False))\n",
        "\n",
        "    kwic_list = []\n",
        "    doc_series.progress_map(add_kwic)\n",
        "\n",
        "    if print_samples is None or print_samples==0:\n",
        "        return kwic_list\n",
        "    else:\n",
        "        k = min(print_samples, len(kwic_list))\n",
        "        print(f\"{k} random samples out of {len(kwic_list)} \" + \\\n",
        "              f\"contexts for '{keyword}':\")\n",
        "        for sample in random.sample(list(kwic_list), k):\n",
        "            print(re.sub(r'[\\n\\t]', ' ', sample[0])+'  '+ \\\n",
        "                  sample[1]+'  '+\\\n",
        "                  re.sub(r'[\\n\\t]', ' ', sample[2]))"
      ],
      "metadata": {
        "id": "TEgh3QnoMMyV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 함수는 map를 이용해서 각 문서에 add_kwic함수를 적용하고, 키워드 컨텍스트를 반복적으로 수집하여서 단어 빈도 분석에서 이미 사용한 이 빕버은 매우 효율적이므로 더 큰 말뭉치에서도 수행할 수 있다.\n",
        "\n",
        "<br>\n",
        "\n",
        "**이 함수는 (왼쪽 컨텍스트, 키워드, 오른쪽 컨텍스트)형태의 튜플 리스트를 반환한다.**\n",
        "\n",
        ">print_sample가 0보다 더 크면 무작위로 샘플링한 결과를 출력하고 샘플링은 주로 목록의 첫째 항목이 단일 또는 매우 적은 수의 문서를 추출해 진행하기 때문에 특히 많은 뭄문서 작업을 할 때 유용하다."
      ],
      "metadata": {
        "id": "INrc2IJlRzjb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "kwic(un[un['year'] == 2015]['text'], 'sdgs', print_samples=5)"
      ],
      "metadata": {
        "id": "uir72DBjQBGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMidfMm_SPnV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}